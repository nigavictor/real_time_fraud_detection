{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Analysis Notebook\n",
    "\n",
    "This notebook provides an interactive environment for exploring the fraud detection dataset and analyzing model results.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "3. [Feature Analysis](#features)\n",
    "4. [Model Performance Analysis](#models)\n",
    "5. [Business Impact Analysis](#business)\n",
    "6. [Advanced Analysis](#advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Import custom modules\n",
    "from src.data_generation.synthetic_data_generator import FraudDataGenerator\n",
    "from src.preprocessing.data_processor import FraudDataProcessor\n",
    "from src.modeling.model_trainer import FraudModelTrainer\n",
    "from src.evaluation.model_evaluator import FraudModelEvaluator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = '../data/raw/'\n",
    "PROCESSED_PATH = '../data/processed/'\n",
    "MODELS_PATH = '../data/models/'\n",
    "\n",
    "# Find the most recent dataset\n",
    "import glob\n",
    "csv_files = glob.glob(f\"{DATA_PATH}*.csv\")\n",
    "if csv_files:\n",
    "    latest_file = max(csv_files, key=os.path.getctime)\n",
    "    print(f\"Loading dataset: {latest_file}\")\n",
    "    df = pd.read_csv(latest_file)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"No dataset found. Please run the data generation pipeline first.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis {#eda}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Basic statistics\n",
    "    print(\"Dataset Overview\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total transactions: {len(df):,}\")\n",
    "    print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"Unique users: {df['user_id'].nunique():,}\")\n",
    "    print(f\"Unique merchants: {df['merchant_category'].nunique()}\")\n",
    "    print(f\"Countries: {df['country'].nunique()}\")\n",
    "    \n",
    "    print(\"\\nFraud Distribution:\")\n",
    "    fraud_counts = df['is_fraud'].value_counts()\n",
    "    fraud_pct = df['is_fraud'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for label, count, pct in zip(['Legitimate', 'Fraudulent'], fraud_counts, fraud_pct):\n",
    "        print(f\"  {label}: {count:,} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Transaction amount distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Amount distribution\n",
    "    axes[0,0].hist(df[df['is_fraud']==0]['amount'], bins=50, alpha=0.7, label='Legitimate', density=True)\n",
    "    axes[0,0].hist(df[df['is_fraud']==1]['amount'], bins=50, alpha=0.7, label='Fraudulent', density=True)\n",
    "    axes[0,0].set_xlabel('Transaction Amount')\n",
    "    axes[0,0].set_ylabel('Density')\n",
    "    axes[0,0].set_title('Transaction Amount Distribution')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].set_xlim(0, df['amount'].quantile(0.95))\n",
    "    \n",
    "    # Log amount distribution\n",
    "    axes[0,1].hist(np.log1p(df[df['is_fraud']==0]['amount']), bins=50, alpha=0.7, label='Legitimate', density=True)\n",
    "    axes[0,1].hist(np.log1p(df[df['is_fraud']==1]['amount']), bins=50, alpha=0.7, label='Fraudulent', density=True)\n",
    "    axes[0,1].set_xlabel('Log(Transaction Amount + 1)')\n",
    "    axes[0,1].set_ylabel('Density')\n",
    "    axes[0,1].set_title('Log Transaction Amount Distribution')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Transaction timing\n",
    "    df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "    hour_fraud = df.groupby(['hour', 'is_fraud']).size().unstack(fill_value=0)\n",
    "    hour_fraud_pct = hour_fraud.div(hour_fraud.sum(axis=1), axis=0)\n",
    "    \n",
    "    axes[1,0].plot(hour_fraud_pct.index, hour_fraud_pct[1], marker='o', color='red', linewidth=2)\n",
    "    axes[1,0].set_xlabel('Hour of Day')\n",
    "    axes[1,0].set_ylabel('Fraud Rate')\n",
    "    axes[1,0].set_title('Fraud Rate by Hour of Day')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Merchant category fraud rate\n",
    "    merchant_fraud = df.groupby('merchant_category')['is_fraud'].agg(['count', 'sum'])\n",
    "    merchant_fraud['fraud_rate'] = merchant_fraud['sum'] / merchant_fraud['count']\n",
    "    merchant_fraud = merchant_fraud.sort_values('fraud_rate', ascending=True)\n",
    "    \n",
    "    axes[1,1].barh(range(len(merchant_fraud)), merchant_fraud['fraud_rate'], color='coral')\n",
    "    axes[1,1].set_yticks(range(len(merchant_fraud)))\n",
    "    axes[1,1].set_yticklabels(merchant_fraud.index, fontsize=8)\n",
    "    axes[1,1].set_xlabel('Fraud Rate')\n",
    "    axes[1,1].set_title('Fraud Rate by Merchant Category')\n",
    "    axes[1,1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis {#features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Create interactive geographic visualization\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        # Sample data for better performance\n",
    "        sample_size = min(5000, len(df))\n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        fig = px.scatter_mapbox(\n",
    "            df_sample,\n",
    "            lat='latitude',\n",
    "            lon='longitude',\n",
    "            color='is_fraud',\n",
    "            color_discrete_map={0: 'blue', 1: 'red'},\n",
    "            hover_data=['amount', 'merchant_category', 'country'],\n",
    "            mapbox_style='open-street-map',\n",
    "            title='Geographic Distribution of Transactions',\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            mapbox=dict(\n",
    "                center=dict(lat=40, lon=-95),\n",
    "                zoom=2\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # User behavior analysis\n",
    "    user_stats = df.groupby('user_id').agg({\n",
    "        'amount': ['count', 'sum', 'mean', 'std'],\n",
    "        'is_fraud': 'sum',\n",
    "        'merchant_category': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    user_stats.columns = ['user_id', 'txn_count', 'total_amount', 'avg_amount', 'std_amount', 'fraud_count', 'unique_merchants']\n",
    "    user_stats['fraud_rate'] = user_stats['fraud_count'] / user_stats['txn_count']\n",
    "    \n",
    "    # Plot user behavior patterns\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Transaction count distribution\n",
    "    axes[0,0].hist(user_stats['txn_count'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0,0].set_xlabel('Transactions per User')\n",
    "    axes[0,0].set_ylabel('Number of Users')\n",
    "    axes[0,0].set_title('Distribution of Transactions per User')\n",
    "    axes[0,0].axvline(user_stats['txn_count'].mean(), color='red', linestyle='--', label=f'Mean: {user_stats[\"txn_count\"].mean():.1f}')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Average amount vs fraud rate\n",
    "    scatter = axes[0,1].scatter(user_stats['avg_amount'], user_stats['fraud_rate'], \n",
    "                               alpha=0.6, c=user_stats['txn_count'], cmap='viridis')\n",
    "    axes[0,1].set_xlabel('Average Transaction Amount')\n",
    "    axes[0,1].set_ylabel('User Fraud Rate')\n",
    "    axes[0,1].set_title('Average Amount vs Fraud Rate by User')\n",
    "    plt.colorbar(scatter, ax=axes[0,1], label='Transaction Count')\n",
    "    \n",
    "    # Merchant diversity vs fraud\n",
    "    axes[1,0].scatter(user_stats['unique_merchants'], user_stats['fraud_rate'], alpha=0.6)\n",
    "    axes[1,0].set_xlabel('Number of Unique Merchants')\n",
    "    axes[1,0].set_ylabel('User Fraud Rate')\n",
    "    axes[1,0].set_title('Merchant Diversity vs Fraud Rate')\n",
    "    \n",
    "    # Fraud users vs normal users\n",
    "    fraud_users = user_stats[user_stats['fraud_count'] > 0]\n",
    "    normal_users = user_stats[user_stats['fraud_count'] == 0]\n",
    "    \n",
    "    axes[1,1].boxplot([normal_users['avg_amount'], fraud_users['avg_amount']], \n",
    "                      labels=['Normal Users', 'Users with Fraud'])\n",
    "    axes[1,1].set_ylabel('Average Transaction Amount')\n",
    "    axes[1,1].set_title('Average Transaction Amount: Normal vs Fraud Users')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Users with fraud: {len(fraud_users):,} ({len(fraud_users)/len(user_stats)*100:.1f}%)\")\n",
    "    print(f\"Average transactions per user: {user_stats['txn_count'].mean():.1f}\")\n",
    "    print(f\"Average amount per user: ${user_stats['avg_amount'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Performance Analysis {#models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model evaluation results if available\n",
    "model_dirs = glob.glob(f\"{MODELS_PATH}trained_models_*\")\n",
    "if model_dirs:\n",
    "    latest_model_dir = max(model_dirs, key=os.path.getctime)\n",
    "    print(f\"Loading models from: {latest_model_dir}\")\n",
    "    \n",
    "    # Check for metadata file\n",
    "    metadata_file = os.path.join(latest_model_dir, 'model_metadata.json')\n",
    "    if os.path.exists(metadata_file):\n",
    "        import json\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"Available models: {metadata['models']}\")\n",
    "        \n",
    "        # Extract evaluation results if available\n",
    "        if 'evaluation_results' in metadata:\n",
    "            eval_results = metadata['evaluation_results']\n",
    "            \n",
    "            # Create performance comparison\n",
    "            if eval_results:\n",
    "                performance_data = []\n",
    "                for result in eval_results:\n",
    "                    if 'error' not in result:\n",
    "                        performance_data.append({\n",
    "                            'model': result['model_name'],\n",
    "                            'roc_auc': result['roc_auc'],\n",
    "                            'f1_score': result['f1_score'],\n",
    "                            'precision': result['precision'],\n",
    "                            'recall': result['recall'],\n",
    "                            'fraud_detection_rate': result['fraud_detection_rate']\n",
    "                        })\n",
    "                \n",
    "                if performance_data:\n",
    "                    perf_df = pd.DataFrame(performance_data)\n",
    "                    \n",
    "                    # Interactive performance comparison\n",
    "                    fig = go.Figure()\n",
    "                    \n",
    "                    metrics = ['roc_auc', 'f1_score', 'precision', 'recall', 'fraud_detection_rate']\n",
    "                    colors = ['blue', 'green', 'red', 'orange', 'purple']\n",
    "                    \n",
    "                    for i, metric in enumerate(metrics):\n",
    "                        fig.add_trace(go.Bar(\n",
    "                            name=metric.replace('_', ' ').title(),\n",
    "                            x=perf_df['model'],\n",
    "                            y=perf_df[metric],\n",
    "                            marker_color=colors[i],\n",
    "                            visible=True if i == 0 else False\n",
    "                        ))\n",
    "                    \n",
    "                    # Add buttons for metric selection\n",
    "                    buttons = []\n",
    "                    for i, metric in enumerate(metrics):\n",
    "                        visibility = [False] * len(metrics)\n",
    "                        visibility[i] = True\n",
    "                        buttons.append(dict(\n",
    "                            label=metric.replace('_', ' ').title(),\n",
    "                            method='update',\n",
    "                            args=[{'visible': visibility}]\n",
    "                        ))\n",
    "                    \n",
    "                    fig.update_layout(\n",
    "                        title='Model Performance Comparison',\n",
    "                        xaxis_title='Model',\n",
    "                        yaxis_title='Score',\n",
    "                        updatemenus=[dict(\n",
    "                            type='buttons',\n",
    "                            direction='left',\n",
    "                            x=0.7,\n",
    "                            y=1.02,\n",
    "                            showactive=True,\n",
    "                            buttons=buttons\n",
    "                        )]\n",
    "                    )\n",
    "                    \n",
    "                    fig.show()\n",
    "                    \n",
    "                    # Display performance table\n",
    "                    print(\"\\nModel Performance Summary:\")\n",
    "                    display(perf_df.round(3))\n",
    "                    \n",
    "                    # Best model recommendation\n",
    "                    best_auc = perf_df.loc[perf_df['roc_auc'].idxmax()]\n",
    "                    best_f1 = perf_df.loc[perf_df['f1_score'].idxmax()]\n",
    "                    \n",
    "                    print(f\"\\nBest ROC AUC: {best_auc['model']} ({best_auc['roc_auc']:.3f})\")\n",
    "                    print(f\"Best F1 Score: {best_f1['model']} ({best_f1['f1_score']:.3f})\")\n",
    "else:\n",
    "    print(\"No trained models found. Please run the model training pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Business Impact Analysis {#business}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact simulation\n",
    "def calculate_business_impact(df, fraud_detection_rate, false_positive_rate, \n",
    "                            avg_fraud_amount=500, investigation_cost=50):\n",
    "    \"\"\"\n",
    "    Calculate business impact of fraud detection system.\n",
    "    \"\"\"\n",
    "    total_transactions = len(df)\n",
    "    total_fraud = df['is_fraud'].sum()\n",
    "    total_legit = total_transactions - total_fraud\n",
    "    \n",
    "    # Model performance\n",
    "    fraud_detected = int(total_fraud * fraud_detection_rate)\n",
    "    fraud_missed = total_fraud - fraud_detected\n",
    "    false_positives = int(total_legit * false_positive_rate)\n",
    "    \n",
    "    # Financial impact\n",
    "    prevented_loss = fraud_detected * avg_fraud_amount\n",
    "    missed_loss = fraud_missed * avg_fraud_amount\n",
    "    investigation_costs = (fraud_detected + false_positives) * investigation_cost\n",
    "    \n",
    "    net_benefit = prevented_loss - investigation_costs\n",
    "    total_potential_loss = total_fraud * avg_fraud_amount\n",
    "    savings_rate = net_benefit / total_potential_loss if total_potential_loss > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_transactions': total_transactions,\n",
    "        'total_fraud': total_fraud,\n",
    "        'fraud_detected': fraud_detected,\n",
    "        'fraud_missed': fraud_missed,\n",
    "        'false_positives': false_positives,\n",
    "        'prevented_loss': prevented_loss,\n",
    "        'missed_loss': missed_loss,\n",
    "        'investigation_costs': investigation_costs,\n",
    "        'net_benefit': net_benefit,\n",
    "        'savings_rate': savings_rate\n",
    "    }\n",
    "\n",
    "if df is not None and 'performance_data' in locals():\n",
    "    print(\"Business Impact Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calculate business impact for each model\n",
    "    business_results = []\n",
    "    \n",
    "    for _, model in perf_df.iterrows():\n",
    "        impact = calculate_business_impact(\n",
    "            df, \n",
    "            model['fraud_detection_rate'],\n",
    "            1 - model['precision'] if model['precision'] > 0 else 0.1  # Approximate FPR\n",
    "        )\n",
    "        impact['model'] = model['model']\n",
    "        business_results.append(impact)\n",
    "    \n",
    "    # Create business impact DataFrame\n",
    "    business_df = pd.DataFrame(business_results)\n",
    "    \n",
    "    # Visualize business impact\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Net benefit comparison\n",
    "    axes[0,0].bar(business_df['model'], business_df['net_benefit'], color='green', alpha=0.7)\n",
    "    axes[0,0].set_xlabel('Model')\n",
    "    axes[0,0].set_ylabel('Net Benefit ($)')\n",
    "    axes[0,0].set_title('Net Financial Benefit by Model')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Prevented vs missed loss\n",
    "    x = np.arange(len(business_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0,1].bar(x - width/2, business_df['prevented_loss'], width, label='Prevented Loss', color='green', alpha=0.7)\n",
    "    axes[0,1].bar(x + width/2, business_df['missed_loss'], width, label='Missed Loss', color='red', alpha=0.7)\n",
    "    axes[0,1].set_xlabel('Model')\n",
    "    axes[0,1].set_ylabel('Loss ($)')\n",
    "    axes[0,1].set_title('Prevented vs Missed Fraud Loss')\n",
    "    axes[0,1].set_xticks(x)\n",
    "    axes[0,1].set_xticklabels(business_df['model'], rotation=45)\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Fraud detection vs false positives\n",
    "    axes[1,0].scatter(business_df['false_positives'], business_df['fraud_detected'], \n",
    "                     s=100, alpha=0.7, c=business_df['net_benefit'], cmap='RdYlGn')\n",
    "    axes[1,0].set_xlabel('False Positives')\n",
    "    axes[1,0].set_ylabel('Fraud Detected')\n",
    "    axes[1,0].set_title('Fraud Detection vs False Positives')\n",
    "    \n",
    "    for i, model in enumerate(business_df['model']):\n",
    "        axes[1,0].annotate(model, (business_df['false_positives'].iloc[i], \n",
    "                                  business_df['fraud_detected'].iloc[i]),\n",
    "                          xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # Savings rate\n",
    "    axes[1,1].bar(business_df['model'], business_df['savings_rate'], color='blue', alpha=0.7)\n",
    "    axes[1,1].set_xlabel('Model')\n",
    "    axes[1,1].set_ylabel('Savings Rate')\n",
    "    axes[1,1].set_title('Savings Rate by Model')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    axes[1,1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display business impact summary\n",
    "    print(\"\\nBusiness Impact Summary:\")\n",
    "    display_cols = ['model', 'fraud_detected', 'false_positives', 'prevented_loss', 'missed_loss', 'net_benefit', 'savings_rate']\n",
    "    display_business = business_df[display_cols].copy()\n",
    "    \n",
    "    # Format currency columns\n",
    "    for col in ['prevented_loss', 'missed_loss', 'net_benefit']:\n",
    "        display_business[col] = display_business[col].apply(lambda x: f\"${x:,.0f}\")\n",
    "    \n",
    "    display_business['savings_rate'] = display_business['savings_rate'].apply(lambda x: f\"{x:.1%}\")\n",
    "    \n",
    "    display(display_business)\n",
    "    \n",
    "    # Best model for business\n",
    "    best_business_model = business_df.loc[business_df['net_benefit'].idxmax()]\n",
    "    print(f\"\\nBest model for business: {best_business_model['model']}\")\n",
    "    print(f\"Net benefit: ${best_business_model['net_benefit']:,.0f}\")\n",
    "    print(f\"Savings rate: {best_business_model['savings_rate']:.1%}\")\n",
    "else:\n",
    "    print(\"Business impact analysis requires model performance data.\")\n",
    "    print(\"Please run the model training and evaluation pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Analysis {#advanced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold analysis\n",
    "def threshold_analysis(y_true, y_scores, thresholds=None):\n",
    "    \"\"\"\n",
    "    Analyze model performance across different thresholds.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.01, 1.0, 0.01)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "        \n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        # Business metrics\n",
    "        tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "        fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "        fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "        tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
    "        \n",
    "        fraud_detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'fraud_detection_rate': fraud_detection_rate,\n",
    "            'false_positive_rate': false_positive_rate\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example threshold analysis (would need actual model predictions)\n",
    "print(\"Threshold Analysis Framework\")\n",
    "print(\"=\"*50)\n",
    "print(\"This section would analyze optimal thresholds for each model based on:\")\n",
    "print(\"- Business objectives (maximize savings vs minimize false positives)\")\n",
    "print(\"- Operational constraints (investigation capacity)\")\n",
    "print(\"- Risk tolerance (acceptable false positive rate)\")\n",
    "print(\"\\nTo run threshold analysis, load model predictions and use the threshold_analysis() function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis placeholder\n",
    "print(\"Feature Importance Analysis\")\n",
    "print(\"=\"*50)\n",
    "print(\"Key features for fraud detection (based on domain knowledge):\")\n",
    "\n",
    "important_features = [\n",
    "    (\"amount_deviation_from_user_avg\", \"Deviation from user's normal spending pattern\"),\n",
    "    (\"travel_velocity_kmh\", \"Speed between consecutive transactions\"),\n",
    "    (\"user_amount_sum_1h\", \"Total amount spent in last hour\"),\n",
    "    (\"is_night\", \"Transaction during night hours (11PM-6AM)\"),\n",
    "    (\"amount_vs_user_max\", \"Current amount vs user's historical maximum\"),\n",
    "    (\"user_txn_count_1h\", \"Number of transactions in last hour\"),\n",
    "    (\"distance_from_prev_km\", \"Distance from previous transaction\"),\n",
    "    (\"is_unusual_category\", \"Merchant category unusual for user\")\n",
    "]\n",
    "\n",
    "for i, (feature, description) in enumerate(important_features, 1):\n",
    "    print(f\"{i}. {feature}: {description}\")\n",
    "\n",
    "print(\"\\nTo see actual feature importance, run the complete pipeline with model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FRAUD DETECTION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"Dataset: {len(df):,} transactions from {df['user_id'].nunique():,} users\")\n",
    "    print(f\"Fraud rate: {df['is_fraud'].mean()*100:.2f}%\")\n",
    "    print(f\"Average transaction: ${df['amount'].mean():.2f}\")\n",
    "    print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"1. Fraud patterns are clearly distinguishable from legitimate transactions\")\n",
    "print(\"2. Time-based features (hour, velocity) are critical for detection\")\n",
    "print(\"3. User behavioral deviation is the strongest fraud indicator\")\n",
    "print(\"4. Geographic analysis helps catch impossible travel scenarios\")\n",
    "print(\"5. Ensemble methods typically provide best overall performance\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"1. Use XGBoost or ensemble model for best performance\")\n",
    "print(\"2. Optimize threshold based on business objectives\")\n",
    "print(\"3. Monitor model performance and retrain regularly\")\n",
    "print(\"4. Implement real-time feature computation for production\")\n",
    "print(\"5. Consider model interpretability for regulatory compliance\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Deploy selected model to production environment\")\n",
    "print(\"2. Set up monitoring and alerting systems\")\n",
    "print(\"3. Implement feedback loop for continuous learning\")\n",
    "print(\"4. Prepare model documentation for stakeholders\")\n",
    "print(\"5. Plan for regular model updates and maintenance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}